<!DOCTYPE HTML>

<html>

	<head>
		<title>melanie tosik - portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link href='https://fonts.googleapis.com/css?family=Playfair+Display:400,700' rel='stylesheet' type='text/css'>
  		<link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
	</head>

	<body>

		<!-- Wrapper -->
		<div id="wrapper">

			<!-- Header -->
			<header id="header">
				<h1>Portfolio</h1>
				<p>Details on previous work and projects</p>
			</header>

			<!-- Main -->
			<div id="main">

				<!-- Content -->
				<section id="content" class="main">

					<h2 id="cpp">Edit distances and sequence alignment</h2>
	              	<p>During my senior year, I finally took a class on advanced C++. Surprisingly enough, it didn't seem nearly as hard as the first one I had to struggle through a few years ago, and I ended up having a lot of fun with it.
	              	As final project, I decided to work on edit distances, and implemented the <a href="https://en.wikipedia.org/wiki/Wagner%E2%80%93Fischer_algorithm">Wagnerâ€“Fischer algorithm</a> as an instance of dynamic programming. Later on, I expanded the project to also cover the <a href="https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm">Needleman-Wunsch algorithm</a> for global sequence alignment.</p>
					<ul class="actions">
						<li><a href="https://github.com/melanietosik/cpp2/tree/master/sda" class="button small">Code</a></li>
					</ul>

					<h2 id="srl">Semantic Role Labeling using linear-chain CRF</h2>
	              	<p>My very last undergrad project for a class on advanced language modeling, where we discussed the theoretical foundations of <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model">hidden Markov models</a>, the <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm">Viterbi</a> and <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">EM algorithm</a>, <a href="https://en.wikipedia.org/wiki/Log-linear_model">log-linear models</a>, <a href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy#Maximum_entropy_models">maximum entropy models</a> (MEMMs), and as well as <a href="https://en.wikipedia.org/wiki/Conditional_random_field">conditional random fields</a> (CRFs).</p>
					<ul class="actions">
						<li><a href="files/srl.pdf" class="button small">Paper</a></li>
						<li><a href="https://github.com/melanietosik/srl" class="button small">Code</a></li>
					</ul>

					<h2 id="amr">String to semantic graph alignment</h2>
	              	<p>For my undergrad thesis, I started working on semantic parsing: the problem of mapping natural language strings to meaning representations. In order to train a semantic parser for English into <a href="https://www.amr.isi.edu/">Abstract Meaning Representation</a>, we first need to know which phrases in the input sentence invoked which concepts in the corresponding AMR graph. The project aimed at building an English/AMR aligner to solve this task automatically.</p>
					<ul class="actions">
						<li><a href="http://www.isi.edu/natural-language/mt/amr_eng_align.pdf" class="button small">Origin</a></li>
						<li><a href="files/thesis.pdf" class="button small">Thesis</a></li>
						<li><a href="http://github.com/melanietosik/thesis_code" class="button small">Code</a></li>
					</ul>

					<h2 id="dep">Semantic dependency graph parsing</h2>
	              	<p>For a class on semantic dependency graph parsing, I wrote a short script that computes statistics for semantic dependency graphs and generates plots for the distribution of words per <a href="http://en.wikipedia.org/wiki/Directed_graph#Indegree_and_outdegree">indegree and outdegree</a>. As final project, I submitted a comprehensive review on <a href="http://amr.isi.edu/">Abstract Meaning Representation</a> (AMR), a set of English sentences paired with simple, readable semantic representations.</p>
					<ul class="actions">
						<li><a href="files/amr.pdf" class="button small">Paper</a></li>
						<li><a href="https://github.com/melanietosik/dp1" class="button small">Code</a></li>
					</ul>

					<h2 id="tk">Research internship at Textkernel</h2>
	              	<p>In 2014, I was a research intern at <a href="http://www.textkernel.com/">Textkernel</a>, where we explored new methods of improving resume parsing for multi-lingual documents.
              		In order to extract structured information in the form of specific phrases like name or address, we adopted the probabilistic <a href="http://en.wikipedia.org/wiki/Conditional_random_field">conditional random fields</a> (CRF) framework. In addition, we experimented with a novel approach that integrates <a href="https://code.google.com/p/word2vec/">continuous vector representations</a> of words as input features for such a model.</p>
					<ul class="actions">
						<li><a href="files/report_tosik_textkernel.pdf" class="button small">Report</a></li>
						<li><a href="http://www.aclweb.org/anthology/W15-1517" class="button small">Paper</a></li>
						<li><a href="http://www.textkernel.com/2014/12/internships-at-textkernel-melanie-tosik/" class="button small">Interview</a></li>
					</ul>

					<h2 id="ds">Word meaning in context</h2>
	              	<p>For a really great class on <a href="http://en.wikipedia.org/wiki/Distributional_semantics">distributional semantics</a>, I presented a paper on &lsquo;Measuring Distributional Similarity in Context&rsquo; (<a href="http://www.aclweb.org/anthology/D10-1113">Dinu and Lapata, 2010</a>).
              		In a nutshell, they attempt to model the intuition that word meaning is represented as a probability distribution over a set of latent senses, and thus modulated by context. They employ two different models: the first based on <a href="http://en.wikipedia.org/wiki/Non-negative_matrix_factorization">non-negative matrix factorization</a> (NMF), and the second implementing <a href="http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet Allocation</a> (LDA).</p>
					<ul class="actions">
						<li><a href="files/dist_sem.pdf" class="button small">Paper</a></li>
						<li><a href="files/dinulapata.pdf" class="button small">Slides</a></li>
					</ul>

					<h2 id="ling">Lexical semantics</h2>
	              	<p>I studied abroad and <a href="http://www.socsci.ulster.ac.uk/irss/linguistics.html">learned some linguistics</a>:<br>
                	<blockquote>Consider an example where a zombie has died and been reanimated, and John drowns him.</blockquote>
            		Presentation slides may or may not help to understand what is going on.</p>
					<ul class="actions">
						<li><a href="files/beavers.pdf" class="button small">Slides</a></li>
					</ul>

					<h2 id="linsim">Word similarity</h2>
	              	<p>Shortly after I learned that computational semantics is a thing, I implemented word similarity according to <a href="files/lin.pdf">Dekang Lin (1998)</a>.</p>
					<ul class="actions">
						<li><a href="http://github.com/melanietosik/linsim" class="button small">Code</a></li>
					</ul>

					<h2 id="psy">Sentence comprehension</h2>
	              	<p>I took some classes on <a href="http://www.uni-potsdam.de/humfak/hum-forschungsschwerpunkte/forschungscluster-sprache.html">psycholinguistics</a>, where I presented a range of interesting papers, including &lsquo;Expectation-based syntactic comprehension&rsquo; (<a href="http://idiom.ucsd.edu/~rlevy/papers/levy-2008-cognition.pdf">Levy, 2008</a>), and &lsquo;Dependency Locality Theory&rsquo; (DLT) (<a href="http://tedlab.mit.edu/tedlab_website/researchpapers/Gibson_2000_DLT.pdf">Gibson, 2000</a>). Check out the slides below!</p>
					<ul class="actions">
						<li><a href="files/levy.pdf" class="button small">Levy</a></li>
						<li><a href="files/gibson.pdf" class="button small">Gibson</a></li>
					</ul>

				</section>

			</div>
				
			<!-- Footer -->
			<footer id="footer">
				<section>
					<h2>Blog</h2>
					<p>I too shall attempt to express my uniquely unqualified opinions on the internet, using words. Check it out!</p>
					<ul class="actions">
						<li><a href="#" class="button">Go to medium.com</a></li>
					</ul>
				</section>
				<section>
					<h2>Get in touch</h2>
					<p>I'm always excited to hear about speaking engagements, job opportunities, open source projects, or simply a chance to get together and talk science&trade;.</p>
					<ul text-align="center" class="icons">
						<li><a href="mailto:melanie.tosik@gmail.com" class="icon fa-envelope alt"><span class="label">E-Mail</span></a></li>
						<li><a href="https://twitter.com/meltomene" class="icon fa-twitter alt"><span class="label">Twitter</span></a></li>
						<li><a href="https://github.com/melanietosik" class="icon fa-github alt"><span class="label">GitHub</span></a></li>
						<li><a href="https://www.linkedin.com/in/melanietosik/" class="icon fa-linkedin alt"><span class="label">LinkedIn</span></a></li>
						<li><a href="https://medium.com/@melanietosik" class="icon fa-medium alt"><span class="label">Medium</span></a></li>
					</ul>
				</section>
				<p class="copyright">&copy; Melanie Tosik. Design credits: <a href="https://html5up.net">HTML5 UP</a>.</p>
			</footer>

		</div>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/jquery.scrollex.min.js"></script>
		<script src="assets/js/jquery.scrolly.min.js"></script>
		<script src="assets/js/skel.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>

	</body>
</html>