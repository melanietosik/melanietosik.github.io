<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Melanie Tosik</title>
    <description>NLP and computer science</description>
    <link>http://localhost:4000</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>NLP with representation learning</title>
        <description>&lt;p&gt;This fall semester, I finally got into “Natural Language Processing with Representation Learning”, a popular course taught by &lt;a href=&quot;http://www.kyunghyuncho.me/&quot;&gt;Kyunghyun Cho&lt;/a&gt; at NYU. I remember reading the lecture note a few years ago and feeling really inspired to learn, so this was a particularly excited class for me.&lt;/p&gt;

&lt;p&gt;If you’re unfamiliar with the topic, I highly recommend taking a look at the course materials yourself. The &lt;a href=&quot;https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf&quot;&gt;lecture note is available on GitHub&lt;/a&gt;, and the &lt;a href=&quot;https://docs.google.com/document/d/1o0TTWocbkqPa9qsTCXnEFXf3NZzwZLLLSw7SSZmNla8/edit#&quot;&gt;syllabus is open-source&lt;/a&gt; as well.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Throughout the course, I completed two large programming assignments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://localhost:4000/files/bow_document_classification.pdf&quot;&gt;“Bag-of-words (BOW) sentiment classification”&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/melanietosik/bow-sentiment-classifier&quot; class=&quot;pa1 tc ba br2 db&quot;&gt;View sentiment project on GitHub ☺︎&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://localhost:4000/files/CNN_RNN_nl_inference.pdf&quot;&gt;“RNN/CNN-based natural language inference”&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/melanietosik/nl-inference&quot; class=&quot;pa1 tc ba br2 db&quot;&gt;View inference project on GitHub ☺︎&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;For our final project, we had to build a sequence of neural machine translation systems for two language pairs, Vietnamese (Vi) →︎ English (En) and Chinese (Zh) →︎ English (En). Specifically, we implemented and evaluated the following model architectures:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RNN based encoder-decoder without attention&lt;/li&gt;
  &lt;li&gt;RNN based encoder-decoder with attention&lt;/li&gt;
  &lt;li&gt;CNN-RNN encoder-decoder with attention&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our results were promising despite the limited amount of training data! You can read all about our work in our project report on &lt;a href=&quot;http://localhost:4000/files/nmt_final_project.pdf&quot;&gt;“RNN/CNN-based Neural Machine Translation for Vietnamese and Chinese to English”&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ds1011teamproject/translation&quot; class=&quot;pa2 tc ba br2 db&quot;&gt;View translation project on GitHub ☺︎&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Fri, 14 Dec 2018 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/posts/nlp-with-repl</link>
        <guid isPermaLink="true">http://localhost:4000/posts/nlp-with-repl</guid>
      </item>
    
      <item>
        <title>HPC for ML</title>
        <description>&lt;p&gt;One of the main reasons why ML (and deep learning in particular) is so successful now is our ability to perform intensive computations on very large amounts of training data. So, I decided to step outside of my comfort zone this semester and take an entire class on supercomputing software, and how it is applied to achieve maximum performance of ML algorithms.&lt;/p&gt;

&lt;p&gt;In this class, we studied how to…&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;use HPC techniques to find and solve performance bottlenecks,&lt;/li&gt;
  &lt;li&gt;do performance measurements and profiling of ML software,&lt;/li&gt;
  &lt;li&gt;evaluate the performance of different ML software stacks and hardware systems,&lt;/li&gt;
  &lt;li&gt;develop high performance distributed ML algorithms, and&lt;/li&gt;
  &lt;li&gt;use fast math libraries, CUDA and C++ to accelerate high-performance ML algorithms.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I definitely learned a lot, and got much more comfortable working with low-level languages.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/melanietosik/hpc&quot; class=&quot;pa2 tc ba br2 db&quot;&gt;View project on GitHub ☺︎&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Mon, 10 Dec 2018 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/posts/hpc-for-ml</link>
        <guid isPermaLink="true">http://localhost:4000/posts/hpc-for-ml</guid>
      </item>
    
      <item>
        <title>Neural networks and quantifier conservativity</title>
        <description>&lt;p&gt;In the spring, I was able to take &lt;a href=&quot;https://www.nyu.edu/projects/bowman/&quot;&gt;Sam Bowman&lt;/a&gt;’s course on “Natural Language Understanding and Computational Semantics”. Here is an excerpt from the course description:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The course will focus on text, but will touch on the full range of applicable techniques for language understanding, including formal logics, statistical methods, distributional methods, and deep learning, and will bring in ideas from formal linguistics where they can be readily used in practice. We’ll discuss tasks like sentiment analysis, word similarity, and question answering, as well as higher level issues like how to effectively represent language meaning.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The &lt;a href=&quot;https://docs.google.com/document/d/1mkB6KA7KuzNeoc9jW3mfOthv_6Uberxs8l2H7BmJdzg/edit&quot;&gt;syllabus is open-source&lt;/a&gt; and still available online.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;As part of the course, we had to produce a substantial new research paper into applied language understanding. Inspired by previous work in psycholinguistics, our group experimented with the learnability bias towards conservative quantifiers that is exhibited by children but not during the training stage of artificial neural networks. Our final paper titled &lt;a href=&quot;https://arxiv.org/abs/1809.05733&quot;&gt;“Neural Networks and Quantifier Conservativity: Does Data Distribution Affect Learnability?”&lt;/a&gt; received overwhelmingly positive feedback and is now available on arXiv.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/mvishwali28/quantifier-rnn-learning&quot; class=&quot;pa2 tc ba br2 db&quot;&gt;View NLU project on GitHub ☺︎&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Sat, 15 Sep 2018 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/posts/nn-and-quantifier-conservativity</link>
        <guid isPermaLink="true">http://localhost:4000/posts/nn-and-quantifier-conservativity</guid>
      </item>
    
      <item>
        <title>Fake news and stance detection</title>
        <description>&lt;p&gt;For the term project of our natural language processing (NLP) class, I collaborated with two other NYU graduate students, &lt;a href=&quot;https://www.antoniomallia.it/&quot;&gt;Antonio Mallia&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/kedarg2005/&quot;&gt;Kedar Gangopadhyay&lt;/a&gt;. We decided to revisit the &lt;a href=&quot;http://www.fakenewschallenge.org/&quot;&gt;Fake News Challenge (FNC-1)&lt;/a&gt; and experiment with different linguistic features to solve the task of stance (and ultimately, fake news) detection.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;You can read all about our work in our paper called &lt;a href=&quot;https://arxiv.org/abs/1808.02831&quot;&gt;“Debunking Fake News One Feature at a Time”&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/NYU-FNC/FakeNewsChallenge&quot; class=&quot;pa3 tc ba br2 db&quot;&gt;View project on GitHub ☺︎&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Wed, 08 Aug 2018 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/posts/fake-news-challenge</link>
        <guid isPermaLink="true">http://localhost:4000/posts/fake-news-challenge</guid>
      </item>
    
      <item>
        <title>Viterbi part-of-speech (POS) tagger</title>
        <description>&lt;p&gt;For my natural language processing (NLP) class, we were asked to implement and train a &lt;a href=&quot;https://en.wikipedia.org/wiki/Part-of-speech_tagging&quot;&gt;part-of-speech (POS) tagger&lt;/a&gt;, as described in the 3rd edition of &lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/10.pdf&quot;&gt;“Speech and Language Processing”&lt;/a&gt; (Jurafsky and Martin). I actually distinctly remember a very similar assignment during my undergraduate NLP class, where I felt so overwhelmed with the data structures and implementation details of the Viterbi algorithm that I ended up skipping the assignment entirely (which was allowed, one time). I’ve always been meaning to revisit this problem since, so I was really excited about finally getting a chance to do so.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In a nutshell, a POS tagger attempts to read in some text in a given language and assign a POS tag to each word (or token), such as &lt;em&gt;noun&lt;/em&gt;, &lt;em&gt;verb&lt;/em&gt;, &lt;em&gt;adjective&lt;/em&gt;, etc. (although in practice these are usually divided into &lt;a href=&quot;https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html&quot;&gt;more fine-grained categories&lt;/a&gt;). This is a hard problem because many (if not most) English words are ambiguous in their POS. For example, the word “experience” frequently occurs as both a noun and a verb, “abstract” can be a noun and an adjective, and so on.&lt;/p&gt;

&lt;p&gt;In general, there are two parts to building a (supervised) POS tagger: training and decoding. For this project, training was done using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Hidden_Markov_model&quot;&gt;hidden Markov model (HMM)&lt;/a&gt;, which essentially involves estimating transition and emission probabilities for the hidden states (POS tags) that correspond to each observation (token) in the annotated training data. To decode new observations, we can then use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Viterbi_algorithm&quot;&gt;Viterbi algorithm&lt;/a&gt;, which computes the most likely sequence of POS tags given the current input sequence/sentence.&lt;/p&gt;

&lt;p&gt;Both the HMM and the Viterbi algorithm are relatively straightforward to implement. The major challenge is how to deal with previously unseen observations for which no probabilities have been observed in the training data. &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_smoothing&quot;&gt;Additive smoothing&lt;/a&gt; is one standard way to deal with unknown words, but there are many more advanced techniques out there that might yield even better results (such as the Good-Turing method, Katz’s backoff, or smoothing by linear interpolation).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/melanietosik/viterbi-pos-tagger&quot; class=&quot;pa3 tc ba br2 db&quot;&gt;View project on GitHub ☺︎&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Sun, 25 Feb 2018 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/posts/Viterbi-POS-tagger</link>
        <guid isPermaLink="true">http://localhost:4000/posts/Viterbi-POS-tagger</guid>
      </item>
    
      <item>
        <title>Text mining in Java</title>
        <description>&lt;p&gt;During my first semester at NYU, I took a class on &lt;a href=&quot;https://en.wikipedia.org/wiki/Predictive_analytics&quot;&gt;predictive analytics&lt;/a&gt;. We covered everything from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining&quot;&gt;data mining project life cycle&lt;/a&gt;, understanding and preprocessing (noisy) data sets, &lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot;&gt;dimensionality reduction&lt;/a&gt;, feature selection, data clustering and classification algorithms, to mining association rules and getting some hands-on experience with large-scale data analytics frameworks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/MapReduce&quot;&gt;MapReduce&lt;/a&gt; in &lt;a href=&quot;http://hadoop.apache.org/&quot;&gt;Hadoop&lt;/a&gt; and &lt;a href=&quot;https://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Throughout the semester, we also designed and implemented a complete text mining pipeline in Java. We started by vectorizing a collection of news articles using &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt;term frequency-inverse document frequency (tf-idf)&lt;/a&gt;. We then used &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;k-means clustering&lt;/a&gt; in combination with measuring the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;&gt;cosine similarity&lt;/a&gt; between the document vectors to cluster the articles based their content and semantic similarity. Finally, we implemented the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm&quot;&gt;k-nearest neighbors algorithm (k-NN)&lt;/a&gt; to assign previously unseen documents to the clusters we had established in step two.&lt;/p&gt;

&lt;p&gt;The only existing libraries I used were &lt;a href=&quot;https://math.nist.gov/javanumerics/jama/&quot;&gt;JAMA&lt;/a&gt; for the &lt;a href=&quot;https://en.wikipedia.org/wiki/Singular-value_decomposition&quot;&gt;singular-value decomposition (SVD)&lt;/a&gt; and &lt;a href=&quot;https://stanfordnlp.github.io/CoreNLP/&quot;&gt;Stanford CoreNLP&lt;/a&gt; to preprocess the text documents. Everything else was implemented from scratch.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/melanietosik/text-mining&quot; class=&quot;pa3 tc ba br2 db&quot;&gt;View project on GitHub ☺︎&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Sat, 27 Jan 2018 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/posts/text-mining-in-Java</link>
        <guid isPermaLink="true">http://localhost:4000/posts/text-mining-in-Java</guid>
      </item>
    
      <item>
        <title>How to get started in NLP</title>
        <description>&lt;p&gt;&lt;img src=&quot;http://localhost:4000/files/displacy.png&quot; alt=&quot;displaCy&quot; title=&quot;Dependency parse tree visualized by displaCy&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Somewhere I read that if you ever have to answer the same question twice, it’s probably a good idea to turn it into a blog post. In keeping with this rule and to save my future self some time, here now my standard answer to the question: “My background is in (some or other) science, and I’m interested in learning NLP. Where do I start?”&lt;/p&gt;

&lt;p&gt;Before you dive in, please note that the list below is really just a very general starting point (and likely incomplete). To help navigate the flood of information, I added short descriptions and difficulty estimates in brackets. Basic programming skills (e.g. in Python) are recommended.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;online-courses&quot;&gt;Online courses&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=nfoudtpBV68&amp;amp;list=PL6397E4B26D00A269&quot;&gt;Dan Jurafsky &amp;amp; Chris Manning: Natural Language Processing&lt;/a&gt;&lt;br /&gt; [great intro video series]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://cs224d.stanford.edu/syllabus.html&quot;&gt;Stanford CS224d: Deep Learning for Natural Language Processing&lt;/a&gt;&lt;br /&gt; [more advanced ML algorithms, deep learning, and NN architectures for NLP]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.coursera.org/learn/natural-language-processing&quot;&gt;Coursera: Introduction to Natural Language Processing&lt;/a&gt;&lt;br /&gt; [intro NLP course offered by the University of Michigan]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;libraries-and-open-source&quot;&gt;Libraries and open source&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://spacy.io/&quot;&gt;spaCy&lt;/a&gt;&lt;br /&gt; [Python; emerging open-source library with &lt;a href=&quot;https://spacy.io/usage/spacy-101&quot;&gt;fantastic usage examples&lt;/a&gt;, API documentation, and demo applications]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.nltk.org/&quot;&gt;Natural Language Toolkit (NLTK)&lt;/a&gt;&lt;br /&gt; [Python; practical intro to programming for NLP, mainly used for &lt;a href=&quot;http://www.nltk.org/book/&quot;&gt;teaching&lt;/a&gt;]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://stanfordnlp.github.io/CoreNLP/&quot;&gt;Stanford CoreNLP&lt;/a&gt;&lt;br /&gt; [Java; high-quality analysis toolkit]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;active-blogs&quot;&gt;Active blogs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://nlpers.blogspot.com/&quot;&gt;natural language processing blog&lt;/a&gt; by Hal Daumé&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://languagelog.ldc.upenn.edu/nll/&quot;&gt;Language Log&lt;/a&gt; by Mark Liberman&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://research.googleblog.com/&quot;&gt;Google Research blog&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://explosion.ai/blog/&quot;&gt;Explosion AI blog&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;books&quot;&gt;Books&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot;&gt;Speech and Language Processing&lt;/a&gt; by Daniel Jurafsky and James H. Martin&lt;br /&gt; [classic NLP textbook that covers all the basics, 3rd edition in progress]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://nlp.stanford.edu/fsnlp/&quot;&gt;Foundations of Statistical Natural Language Processing&lt;/a&gt; by Chris Manning and Hinrich Schütze&lt;br /&gt; [more advanced, statistical NLP methods]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://nlp.stanford.edu/IR-book/&quot;&gt;Introduction to Information Retrieval&lt;/a&gt; by Chris Manning, Prabhakar Raghavan and Hinrich Schütze&lt;br /&gt; [excellent reference on ranking/search]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.amazon.com/Network-Methods-Natural-Language-Processing/dp/1627052984&quot;&gt;Neural Network Methods in Natural Language Processing&lt;/a&gt; by Yoav Goldberg&lt;br /&gt; [deep intro to NN approaches to NLP, &lt;a href=&quot;http://u.cs.biu.ac.il/~yogo/nnlp.pdf&quot;&gt;primer here&lt;/a&gt;]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;miscellaneous&quot;&gt;Miscellaneous&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/versions/master/tutorials/word2vec/index.html&quot;&gt;How to build a word2vec model in TensorFlow&lt;/a&gt;&lt;br /&gt; [tutorial]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/andrewt3000/dl4nlp&quot;&gt;Deep Learning for NLP resources&lt;/a&gt;&lt;br /&gt; [overview of state-of-the-art resources for deep learning, organized by topic]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://mitp.nautil.us/article/170/last-words-computational-linguistics-and-deep-learning&quot;&gt;Last Words: Computational Linguistics and Deep Learning –  A look at the importance of Natural Language Processing.&lt;/a&gt; by Chris Manning&lt;br /&gt; [article]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf&quot;&gt;Natural Language Understanding with Distributed Representation&lt;/a&gt; by Kyunghyun Cho&lt;br /&gt; [self-contained lecture note on ML/NN approaches to NLU]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.isi.edu/natural-language/people/bayes-with-tears.pdf&quot;&gt;Bayesian Inference with Tears&lt;/a&gt; by Kevin Knight&lt;br /&gt; [tutorial workbook]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://aclanthology.info/&quot;&gt;Association for Computational Linguistics&lt;/a&gt; (ACL)&lt;br /&gt; [journal anthology]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.quora.com/How-do-I-learn-Natural-Language-Processing&quot;&gt;Quora: How do I learn Natural Language Processing?&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;diwhy-projects-and-data-sets&quot;&gt;DI(WH)Y projects and data sets&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/files/comic.png&quot; alt=&quot;diwhy&quot; title=&quot;http://gunshowcomic.com/&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A thorough &lt;a href=&quot;https://github.com/niderhoff/nlp-datasets&quot;&gt;list of publicly available NLP data sets&lt;/a&gt; has already been created by Nicolas Iderhoff. Beyond these, here are some projects I can recommend to any NLP novice wanting to get their hands dirty:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Implement a &lt;a href=&quot;https://en.wikipedia.org/wiki/Part-of-speech_tagging&quot;&gt;part-of-speech (POS) tagger&lt;/a&gt; based on a &lt;a href=&quot;https://en.wikipedia.org/wiki/Hidden_Markov_model&quot;&gt;hidden Markov model&lt;/a&gt; (HMM)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implement the &lt;a href=&quot;https://en.wikipedia.org/wiki/CYK_algorithm&quot;&gt;CYK algorithm&lt;/a&gt; for parsing &lt;a href=&quot;https://en.wikipedia.org/wiki/Context-free_grammar&quot;&gt;context-free grammars&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implement &lt;a href=&quot;https://en.wikipedia.org/wiki/Semantic_similarity&quot;&gt;semantic similarity&lt;/a&gt; between two given words in a collection of text, e.g. &lt;a href=&quot;https://en.wikipedia.org/wiki/Pointwise_mutual_information&quot;&gt;pointwise mutual information&lt;/a&gt; (PMI)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implement a &lt;a href=&quot;https://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot;&gt;Naive Bayes classifier&lt;/a&gt; to &lt;a href=&quot;https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering&quot;&gt;filter spam&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implement a &lt;a href=&quot;https://en.wikipedia.org/wiki/Spell_checker&quot;&gt;spell checker&lt;/a&gt; based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Edit_distance&quot;&gt;edit distances&lt;/a&gt; between words&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implement a &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_chain&quot;&gt;Markov chain&lt;/a&gt; text generator&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implement a &lt;a href=&quot;https://en.wikipedia.org/wiki/Topic_model&quot;&gt;topic model&lt;/a&gt; using &lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&quot;&gt;latent Dirichlet allocation&lt;/a&gt; (LDA)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use &lt;a href=&quot;https://code.google.com/archive/p/word2vec/&quot;&gt;word2vec&lt;/a&gt; to generate word embeddings from a large text corpus, e.g. &lt;a href=&quot;https://en.wikipedia.org/wiki/Wikipedia:Database_download&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;k-means&lt;/a&gt; to cluster &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt;tf-idf&lt;/a&gt; vectors of text, e.g. news articles&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;nlp-on-social-media&quot;&gt;NLP on social media&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Twitter: &lt;a href=&quot;https://twitter.com/jasonbaldridge/lists/nlpers&quot;&gt;List of NLPers&lt;/a&gt; by Jason Baldrige and &lt;code class=&quot;highlighter-rouge&quot;&gt;#nlproc&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reddit: &lt;code class=&quot;highlighter-rouge&quot;&gt;r/LanguageTechnology&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;!!!&lt;/strong&gt; For a more up-to-date version of this post, visit the article on &lt;a href=&quot;https://towardsdatascience.com/how-to-get-started-in-nlp-6a62aa4eaeff&quot;&gt;medium.com&lt;/a&gt;. &lt;strong&gt;!!!&lt;/strong&gt;&lt;/p&gt;

</description>
        <pubDate>Mon, 01 May 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/posts/how-to-get-started-in-NLP</link>
        <guid isPermaLink="true">http://localhost:4000/posts/how-to-get-started-in-NLP</guid>
      </item>
    
      <item>
        <title>Thoughts on getting into graduate school</title>
        <description>&lt;p&gt;Three years after I applied to graduate school for the very first time, I finally received an email with the word “admission” in the subject line. It was easily one of the most joyous moments of my professional career. And despite the 150 million Google search results on “how to get into graduate school”, I’d like to take a moment to reflect on where I was, how I got here, and where I’d like to go in the future. Hopefully documenting my own story will one day inspire somebody else out there to keep rewriting theirs.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;where-i-was&quot;&gt;Where I was&lt;/h3&gt;

&lt;p&gt;Straight out of high school, I had made the decision that I wanted to study Computational Linguistics. After mostly regular coursework for a few semesters, I realized how much more there was to academia when I (coincidentally) ended up helping out at &lt;a href=&quot;http://www.ling.uni-potsdam.de/iwcs2013/&quot;&gt;a NLP conference&lt;/a&gt; that just happened to be held at my university. Over the course of just a few days, I met more people from around the world than I could count, and &lt;em&gt;all of them&lt;/em&gt; were trying to further the exact field I had studied for the past two years!&lt;/p&gt;

&lt;p&gt;Back at school, my attitude slowly shifted from just wanting to do well in classes to sooner or later becoming part of that same dedicated and encouraging community of researchers. In order to do so, I quit my side job as a system administrator and took on research and teaching assistantships instead. I volunteered at a handful of other conferences, and even managed to turn my summer internship work into a half-decent publication of my own. Nearing the end of my junior year, I was dead set on graduate school being the one and only path forward, and I didn’t see any reason why it shouldn’t work out this way.&lt;/p&gt;

&lt;p&gt;As it turns out, I was wrong. It was already September before I even realized that most American universities have their deadlines in early December. Scrambling to get my applications together in time, I didn’t spend more than maybe a week of studying before I went to take the GRE, resulting in only slightly above average scores. But who cares about standardized test results anyway, right? Plus, I didn’t have the money to retake the test even if I wanted to, and I was quickly running out of time. My transcript was still incomplete as well, but I figured most applicants wouldn’t have completed their degrees by the deadline either. Last but not least, I threw together a rather desultory statement that was pretty much just a typed up version of my CV, with a sprinkle of buzzwords and noteworthy professors for good measure.&lt;/p&gt;

&lt;p&gt;In the end, I managed to apply to six top-tier U.S. universities, which not only cost me a lot of time I didn’t have in the first place, but also about a thousand dollars I could barely scrape up as a student. Needless to say, I got rejected from every single school I applied to. The entire application process stood in such stark contrast to the bright prospects I had been promised by my peers and advisors. And back then, I had no idea where I went wrong.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;how-i-got-here&quot;&gt;How I got here&lt;/h3&gt;

&lt;p&gt;In hindsight, the admission committees probably made the right call. It’s quite obvious to me now, but I don’t believe I was truly equipped to become a successful PhD student. I was too arrogant to seriously prepare for the GRE, and my statement was neither here nor there. While I was infatuated with the &lt;em&gt;idea&lt;/em&gt; of being a PhD student, I was clearly lacking any real motivation to embark upon a year-long journey of independent research and self-reliance.&lt;/p&gt;

&lt;p&gt;Despite my initial skepticism, spending some time in industry instead turned out to be immensely valuable in gaining some perspective on my career as a whole. In college, the prospect of eventually applying my mainly theoretical knowledge to problems in “the real world” always seemed intimidating. Working for a startup, I learned to overcome my (unsubstantiated) fear of failure, as well as to thrive in an environment that promoted autonomous decision making and creative problem solving. Being able to meaningfully contribute to an expert team of developers right from the beginning was an empowering experience that enabled me to grow not only professionally but personally as well.&lt;/p&gt;

&lt;p&gt;With newfound confidence and inspiration, I decided to give graduate school another try. And this time, I would come prepared!&lt;/p&gt;

&lt;p&gt;The first thing I did was to start studying for the GRE about half a year before applications were due. I even signed up for a small test preparation class that would meet once a week and ultimately help increase my GRE scores by a huge margin. Most importantly though, I had a much better understanding of why I &lt;em&gt;really&lt;/em&gt; wanted to graduate school: to gain an deeper understanding of state-of-the-art methodologies in NLP/ML, and to acquire the relevant skill set to carry out independent research in a large-scale, industrial setting. I also really missed the intellectual freedom of studying at a university and was hoping to get a chance to collaborate on a variety of different research projects in academia.&lt;/p&gt;

&lt;p&gt;After identifying a handful of suitable Masters instead of PhD programs, I was capable of composing a &lt;a href=&quot;http://www.cs.umd.edu/grad/writing-statement-of-pupose&quot;&gt;cohesive, original statement&lt;/a&gt; for each university which, in fact, clearly stated my purpose and motivation. I also made sure to devote more than half of each statement to the particular research groups and faculty I wanted to work with, and details on potential common areas of interest.&lt;/p&gt;

&lt;p&gt;Since I went through the process before, I was able to get ahead of all the common pitfalls and approach the application season level-headed and well prepared. Working a day job was additionally invaluable in that I had both the funds and enough spare time to put together a strong application. In the end, I was able to score an offer of admission from 2 out of the 5 universities I applied to, which to me is a very proud accomplishment.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;where-im-going&quot;&gt;Where I’m going&lt;/h3&gt;

&lt;p&gt;Time will tell! For now, I will be joining the MSCS program at NYU this fall. I am beyond to excited to finally go to graduate school, and I’m very much looking forward to starting a new chapter in New York City. I am hoping to write and publish more as I go along, so please don’t hesitate to share your own thoughts and experiences.&lt;/p&gt;

&lt;p&gt;To anyone still out there trying, know that it is indeed possible to overcome most challenges along the way through tenacity, perseverance, and hard work. Just keep at it and &lt;a href=&quot;http://www.chaosmatrix.org/library/humor/reject.html&quot;&gt;don’t let yourself get down&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Tue, 18 Apr 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/posts/thoughts-on-getting-into-graduate-school</link>
        <guid isPermaLink="true">http://localhost:4000/posts/thoughts-on-getting-into-graduate-school</guid>
      </item>
    
      <item>
        <title>Dish AI API</title>
        <description>&lt;p&gt;At &lt;a href=&quot;https://www.wayblazer.ai/&quot;&gt;WayBlazer&lt;/a&gt;, our product manager kept joking about how we needed an automated system (dubbed “dish AI”) to review our catered lunches every day. So I built it, featuring a preprocessed &lt;a href=&quot;https://www.yelp.com/dataset_challenge&quot;&gt;restaurant review data set&lt;/a&gt;, &lt;a href=&quot;https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html&quot;&gt;topic models&lt;/a&gt;, a &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_chain#Markov_text_generators&quot;&gt;Markov chain generator&lt;/a&gt;, and a &lt;a href=&quot;http://flask.pocoo.org/&quot;&gt;Flask API&lt;/a&gt; to put it all together!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/melanietosik/dish-ai&quot; class=&quot;pa3 tc ba br2 db&quot;&gt;View project on GitHub ☺︎&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Tue, 18 Apr 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/posts/dish-ai</link>
        <guid isPermaLink="true">http://localhost:4000/posts/dish-ai</guid>
      </item>
    
      <item>
        <title>Semantic role labeling using CRF</title>
        <description>&lt;p&gt;My very last undergraduate project for a class on advanced language modeling, where we discussed the theoretical foundations of &lt;a href=&quot;https://en.wikipedia.org/wiki/Hidden_Markov_model&quot;&gt;hidden Markov models&lt;/a&gt; (HMM), the &lt;a href=&quot;https://en.wikipedia.org/wiki/Viterbi_algorithm&quot;&gt;Viterbi&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;EM&lt;/a&gt; algorithm, &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-linear_model&quot;&gt;log-linear models&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Principle_of_maximum_entropy#Maximum_entropy_models&quot;&gt;maximum entropy models&lt;/a&gt; (MEMM), and as well as &lt;a href=&quot;https://en.wikipedia.org/wiki/Conditional_random_field&quot;&gt;conditional random fields&lt;/a&gt; (CRF).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I finished the class with a short paper on &lt;a href=&quot;http://localhost:4000/files/srl.pdf&quot;&gt;“Semantic Role Labeling using Linear-Chain CRF”&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/melanietosik/srl&quot; class=&quot;pa3 tc ba br2 db&quot;&gt;View project on GitHub ☺︎&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Tue, 22 Sep 2015 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/posts/semantic-role-labeling-crf</link>
        <guid isPermaLink="true">http://localhost:4000/posts/semantic-role-labeling-crf</guid>
      </item>
    
  </channel>
</rss>
