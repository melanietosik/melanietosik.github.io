<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Melanie Tosik</title>
    <description>NLP and computer science</description>
    <link>http://localhost:4000</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Fake news and stance detection</title>
        <description>&lt;p&gt;For the term project of our natural language processing (NLP) class, I collaborated with two other NYU graduate students, &lt;a href=&quot;https://www.antoniomallia.it/&quot;&gt;Antonio Mallia&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/kedarg2005/&quot;&gt;Kedar Gangopadhyay&lt;/a&gt;. We decided to revisit the &lt;a href=&quot;http://www.fakenewschallenge.org/&quot;&gt;Fake News Challenge (FNC-1)&lt;/a&gt; and experiment with different linguistic features to solve the task of stance (and ultimately, fake news) detection.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;You can read all about our work in our paper called &lt;a href=&quot;https://arxiv.org/abs/1808.02831&quot;&gt;“Debunking Fake News One Feature at a Time”&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/NYU-FNC/FakeNewsChallenge&quot; class=&quot;pa3 tc ba br2 db&quot;&gt;View project on GitHub ☺&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Wed, 08 Aug 2018 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/posts/fake-news-challenge</link>
        <guid isPermaLink="true">http://localhost:4000/posts/fake-news-challenge</guid>
      </item>
    
      <item>
        <title>Viterbi part-of-speech (POS) tagger</title>
        <description>&lt;p&gt;For my natural language processing (NLP) class, we were asked to implement and train a &lt;a href=&quot;https://en.wikipedia.org/wiki/Part-of-speech_tagging&quot;&gt;part-of-speech (POS) tagger&lt;/a&gt;, as described in the 3rd edition of &lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/10.pdf&quot;&gt;“Speech and Language Processing”&lt;/a&gt; (Jurafsky and Martin). I actually distinctly remember a very similar assignment during my undergraduate NLP class, where I felt so overwhelmed with the data structures and implementation details of the Viterbi algorithm that I ended up skipping the assignment entirely (which was allowed, one time). I’ve always been meaning to revisit this problem since, so I was really excited about finally getting a chance to do so.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In a nutshell, a POS tagger attempts to read in some text in a given language and assign a POS tag to each word (or token), such as &lt;em&gt;noun&lt;/em&gt;, &lt;em&gt;verb&lt;/em&gt;, &lt;em&gt;adjective&lt;/em&gt;, etc. (although in practice these are usually divided into &lt;a href=&quot;https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html&quot;&gt;more fine-grained categories&lt;/a&gt;). This is a hard problem because many (if not most) English words are ambiguous in their POS. For example, the word “experience” frequently occurs as both a noun and a verb, “abstract” can be a noun and an adjective, and so on.&lt;/p&gt;

&lt;p&gt;In general, there are two parts to building a (supervised) POS tagger: training and decoding. For this project, training was done using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Hidden_Markov_model&quot;&gt;hidden Markov model (HMM)&lt;/a&gt;, which essentially involves estimating transition and emission probabilities for the hidden states (POS tags) that correspond to each observation (token) in the annotated training data. To decode new observations, we can then use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Viterbi_algorithm&quot;&gt;Viterbi algorithm&lt;/a&gt;, which computes the most likely sequence of POS tags given the current input sequence/sentence.&lt;/p&gt;

&lt;p&gt;Both the HMM and the Viterbi algorithm are relatively strwaightforward to implement. The major challenge is how to deal with previously unseen observations for which no probabilities have been observed in the training data. &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_smoothing&quot;&gt;Additive smoothing&lt;/a&gt; is one standard way to deal with unknown words, but there are many more advanced techniques out there that might yield even better results (such as the Good-Turing method, Katz’s backoff, or smoothing by linear interpolation).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/melanietosik/viterbi-pos-tagger&quot; class=&quot;pa3 tc ba br2 db&quot;&gt;View project on GitHub ☺&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Sun, 25 Feb 2018 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/posts/Viterbi-POS-tagger</link>
        <guid isPermaLink="true">http://localhost:4000/posts/Viterbi-POS-tagger</guid>
      </item>
    
      <item>
        <title>Text mining in Java</title>
        <description>&lt;p&gt;During my first semester at NYU, I took a class on &lt;a href=&quot;https://en.wikipedia.org/wiki/Predictive_analytics&quot;&gt;predictive analytics&lt;/a&gt;. We covered everything from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining&quot;&gt;data mining project life cycle&lt;/a&gt;, understanding and preprocessing (noisy) data sets, &lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot;&gt;dimensionality reduction&lt;/a&gt;, feature selection, data clustering and classification algorithms, to mining association rules and getting some hands-on experience with large-scale data analytics frameworks, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/MapReduce&quot;&gt;MapReduce&lt;/a&gt; in &lt;a href=&quot;http://hadoop.apache.org/&quot;&gt;Hadoop&lt;/a&gt; and &lt;a href=&quot;https://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Throughout the semester, we also designed and implemented a complete text mining pipeline in Java. We started by vectorizing a collection of news articles using &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt;term frequency-inverse document frequency (tf-idf)&lt;/a&gt;. We then used &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;k-means clustering&lt;/a&gt; in combination with measuring the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;&gt;cosine similarity&lt;/a&gt; between the document vectors to cluster the articles based their content and semantic similarity. Finally, we implemented the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm&quot;&gt;k-nearest neighbors algorithm (k-NN)&lt;/a&gt; to assign previously unseen documents to the clusters we had established in step two.&lt;/p&gt;

&lt;p&gt;The only existing libraries I used were &lt;a href=&quot;https://math.nist.gov/javanumerics/jama/&quot;&gt;JAMA&lt;/a&gt; for the &lt;a href=&quot;https://en.wikipedia.org/wiki/Singular-value_decomposition&quot;&gt;singular-value decomposition (SVD)&lt;/a&gt; and &lt;a href=&quot;https://stanfordnlp.github.io/CoreNLP/&quot;&gt;Stanford CoreNLP&lt;/a&gt; to preprocess the text documents. Everything else was implemented from scratch.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/melanietosik/text-mining&quot; class=&quot;pa3 tc ba br2 db&quot;&gt;View project on GitHub ☺&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Sat, 27 Jan 2018 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/posts/text-mining-in-Java</link>
        <guid isPermaLink="true">http://localhost:4000/posts/text-mining-in-Java</guid>
      </item>
    
      <item>
        <title>How to get started in NLP</title>
        <description>&lt;p&gt;&lt;img src=&quot;http://localhost:4000/files/displacy.png&quot; alt=&quot;displaCy&quot; title=&quot;Dependency parse tree visualized by displaCy&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Somewhere I read that if you ever have to answer the same question twice, it’s probably a good idea to turn it into a blog post. In keeping with this rule and to save my future self some time, here now my standard answer to the question: “My background is in (some or other) science, and I’m interested in learning NLP. Where do I start?”&lt;/p&gt;

&lt;p&gt;Before you dive in, please note that the list below is really just a very general starting point (and likely incomplete). To help navigate the flood of information, I added short descriptions and difficulty estimates in brackets. Basic programming skills (e.g. in Python) are recommended.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;online-courses&quot;&gt;Online courses&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=nfoudtpBV68&amp;amp;list=PL6397E4B26D00A269&quot;&gt;Dan Jurafsky &amp;amp; Chris Manning: Natural Language Processing&lt;/a&gt;&lt;br /&gt; [great intro video series]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://cs224d.stanford.edu/syllabus.html&quot;&gt;Stanford CS224d: Deep Learning for Natural Language Processing&lt;/a&gt;&lt;br /&gt; [more advanced ML algorithms, deep learning, and NN architectures for NLP]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.coursera.org/learn/natural-language-processing&quot;&gt;Coursera: Introduction to Natural Language Processing&lt;/a&gt;&lt;br /&gt; [intro NLP course offered by the University of Michigan]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;libraries-and-open-source&quot;&gt;Libraries and open source&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://spacy.io/&quot;&gt;spaCy&lt;/a&gt;&lt;br /&gt; [Python; emerging open-source library with &lt;a href=&quot;https://spacy.io/usage/spacy-101&quot;&gt;fantastic usage examples&lt;/a&gt;, API documentation, and demo applications]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.nltk.org/&quot;&gt;Natural Language Toolkit (NLTK)&lt;/a&gt;&lt;br /&gt; [Python; practical intro to programming for NLP, mainly used for &lt;a href=&quot;http://www.nltk.org/book/&quot;&gt;teaching&lt;/a&gt;]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://stanfordnlp.github.io/CoreNLP/&quot;&gt;Stanford CoreNLP&lt;/a&gt;&lt;br /&gt; [Java; high-quality analysis toolkit]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;active-blogs&quot;&gt;Active blogs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://nlpers.blogspot.com/&quot;&gt;natural language processing blog&lt;/a&gt; by Hal Daumé&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://languagelog.ldc.upenn.edu/nll/&quot;&gt;Language Log&lt;/a&gt; by Mark Liberman&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://research.googleblog.com/&quot;&gt;Google Research blog&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://explosion.ai/blog/&quot;&gt;Explosion AI blog&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;books&quot;&gt;Books&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot;&gt;Speech and Language Processing&lt;/a&gt; by Daniel Jurafsky and James H. Martin&lt;br /&gt; [classic NLP textbook that covers all the basics, 3rd edition in progress]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://nlp.stanford.edu/fsnlp/&quot;&gt;Foundations of Statistical Natural Language Processing&lt;/a&gt; by Chris Manning and Hinrich Schütze&lt;br /&gt; [more advanced, statistical NLP methods]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://nlp.stanford.edu/IR-book/&quot;&gt;Introduction to Information Retrieval&lt;/a&gt; by Chris Manning, Prabhakar Raghavan and Hinrich Schütze&lt;br /&gt; [excellent reference on ranking/search]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.amazon.com/Network-Methods-Natural-Language-Processing/dp/1627052984&quot;&gt;Neural Network Methods in Natural Language Processing&lt;/a&gt; by Yoav Goldberg&lt;br /&gt; [deep intro to NN approaches to NLP, &lt;a href=&quot;http://u.cs.biu.ac.il/~yogo/nnlp.pdf&quot;&gt;primer here&lt;/a&gt;]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;miscellaneous&quot;&gt;Miscellaneous&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/versions/master/tutorials/word2vec/index.html&quot;&gt;How to build a word2vec model in TensorFlow&lt;/a&gt;&lt;br /&gt; [tutorial]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/andrewt3000/dl4nlp&quot;&gt;Deep Learning for NLP resources&lt;/a&gt;&lt;br /&gt; [overview of state-of-the-art resources for deep learning, organized by topic]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://mitp.nautil.us/article/170/last-words-computational-linguistics-and-deep-learning&quot;&gt;Last Words: Computational Linguistics and Deep Learning –  A look at the importance of Natural Language Processing.&lt;/a&gt; by Chris Manning&lt;br /&gt; [article]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf&quot;&gt;Natural Language Understanding with Distributed Representation&lt;/a&gt; by Kyunghyun Cho&lt;br /&gt; [self-contained lecture note on ML/NN approaches to NLU]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.isi.edu/natural-language/people/bayes-with-tears.pdf&quot;&gt;Bayesian Inference with Tears&lt;/a&gt; by Kevin Knight&lt;br /&gt; [tutorial workbook]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://aclanthology.info/&quot;&gt;Association for Computational Linguistics&lt;/a&gt; (ACL)&lt;br /&gt; [journal anthology]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.quora.com/How-do-I-learn-Natural-Language-Processing&quot;&gt;Quora: How do I learn Natural Language Processing?&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;diwhy-projects-and-data-sets&quot;&gt;DI(WH)Y projects and data sets&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/files/comic.png&quot; alt=&quot;diwhy&quot; title=&quot;http://gunshowcomic.com/&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A thorough &lt;a href=&quot;https://github.com/niderhoff/nlp-datasets&quot;&gt;list of publicly available NLP data sets&lt;/a&gt; has already been created by Nicolas Iderhoff. Beyond these, here are some projects I can recommend to any NLP novice wanting to get their hands dirty:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Implement a &lt;a href=&quot;https://en.wikipedia.org/wiki/Part-of-speech_tagging&quot;&gt;part-of-speech (POS) tagger&lt;/a&gt; based on a &lt;a href=&quot;https://en.wikipedia.org/wiki/Hidden_Markov_model&quot;&gt;hidden Markov model&lt;/a&gt; (HMM)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implement the &lt;a href=&quot;https://en.wikipedia.org/wiki/CYK_algorithm&quot;&gt;CYK algorithm&lt;/a&gt; for parsing &lt;a href=&quot;https://en.wikipedia.org/wiki/Context-free_grammar&quot;&gt;context-free grammars&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implement &lt;a href=&quot;https://en.wikipedia.org/wiki/Semantic_similarity&quot;&gt;semantic similarity&lt;/a&gt; between two given words in a collection of text, e.g. &lt;a href=&quot;https://en.wikipedia.org/wiki/Pointwise_mutual_information&quot;&gt;pointwise mutual information&lt;/a&gt; (PMI)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implement a &lt;a href=&quot;https://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot;&gt;Naive Bayes classifier&lt;/a&gt; to &lt;a href=&quot;https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering&quot;&gt;filter spam&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implement a &lt;a href=&quot;https://en.wikipedia.org/wiki/Spell_checker&quot;&gt;spell checker&lt;/a&gt; based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Edit_distance&quot;&gt;edit distances&lt;/a&gt; between words&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implement a &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_chain&quot;&gt;Markov chain&lt;/a&gt; text generator&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Implement a &lt;a href=&quot;https://en.wikipedia.org/wiki/Topic_model&quot;&gt;topic model&lt;/a&gt; using &lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&quot;&gt;latent Dirichlet allocation&lt;/a&gt; (LDA)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use &lt;a href=&quot;https://code.google.com/archive/p/word2vec/&quot;&gt;word2vec&lt;/a&gt; to generate word embeddings from a large text corpus, e.g. &lt;a href=&quot;https://en.wikipedia.org/wiki/Wikipedia:Database_download&quot;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;k-means&lt;/a&gt; to cluster &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt;tf-idf&lt;/a&gt; vectors of text, e.g. news articles&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;nlp-on-social-media&quot;&gt;NLP on social media&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Twitter: &lt;a href=&quot;https://twitter.com/jasonbaldridge/lists/nlpers&quot;&gt;List of NLPers&lt;/a&gt; by Jason Baldrige and &lt;code class=&quot;highlighter-rouge&quot;&gt;#nlproc&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reddit: &lt;code class=&quot;highlighter-rouge&quot;&gt;r/LanguageTechnology&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

</description>
        <pubDate>Mon, 01 May 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/posts/how-to-get-started-in-NLP</link>
        <guid isPermaLink="true">http://localhost:4000/posts/how-to-get-started-in-NLP</guid>
      </item>
    
      <item>
        <title>Thoughts on getting into graduate school</title>
        <description>&lt;p&gt;Three years after I applied to graduate school for the very first time, I finally received an email with the word “admission” in the subject line. It was easily one of the most joyous moments of my professional career. And despite the 150 million Google search results on “how to get into graduate school”, I’d like to take a moment to reflect on where I was, how I got here, and where I’d like to go in the future. Hopefully documenting my own story will one day inspire somebody else out there to keep rewriting theirs.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;where-i-was&quot;&gt;Where I was&lt;/h3&gt;

&lt;p&gt;Straight out of high school, I had made the decision that I wanted to study Computational Linguistics. After mostly regular coursework for a few semesters, I realized how much more there was to academia when I (coincidentally) ended up helping out at &lt;a href=&quot;http://www.ling.uni-potsdam.de/iwcs2013/&quot;&gt;a NLP conference&lt;/a&gt; that just happened to be held at my university. Over the course of just a few days, I met more people from around the world than I could count, and &lt;em&gt;all of them&lt;/em&gt; were trying to further the exact field I had studied for the past two years!&lt;/p&gt;

&lt;p&gt;Back at school, my attitude slowly shifted from just wanting to do well in classes to sooner or later becoming part of that same dedicated and encouraging community of researchers. In order to do so, I quit my side job as a system administrator and took on research and teaching assistantships instead. I volunteered at a handful of other conferences, and even managed to turn my summer internship work into a half-decent publication of my own. Nearing the end of my junior year, I was dead set on graduate school being the one and only path forward, and I didn’t see any reason why it shouldn’t work out this way.&lt;/p&gt;

&lt;p&gt;As it turns out, I was wrong. It was already September before I even realized that most American universities have their deadlines in early December. Scrambling to get my applications together in time, I didn’t spend more than maybe a week of studying before I went to take the GRE, resulting in only slightly above average scores. But who cares about standardized test results anyway, right? Plus, I didn’t have the money to retake the test even if I wanted to, and I was quickly running out of time. My transcript was still incomplete as well, but I figured most applicants wouldn’t have completed their degrees by the deadline either. Last but not least, I threw together a rather desultory statement that was pretty much just a typed up version of my CV, with a sprinkle of buzzwords and noteworthy professors for good measure.&lt;/p&gt;

&lt;p&gt;In the end, I managed to apply to six top-tier U.S. universities, which not only cost me a lot of time I didn’t have in the first place, but also about a thousand dollars I could barely scrape up as a student. Needless to say, I got rejected from every single school I applied to. The entire application process stood in such stark contrast to the bright prospects I had been promised by my peers and advisors. And back then, I had no idea where I went wrong.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;how-i-got-here&quot;&gt;How I got here&lt;/h3&gt;

&lt;p&gt;In hindsight, the admission committees probably made the right call. It’s quite obvious to me now, but I don’t believe I was truly equipped to become a successful PhD student. I was too arrogant to seriously prepare for the GRE, and my statement was neither here nor there. While I was infatuated with the &lt;em&gt;idea&lt;/em&gt; of being a PhD student, I was clearly lacking any real motivation to embark upon a year-long journey of independent research and self-reliance.&lt;/p&gt;

&lt;p&gt;Despite my initial skepticism, spending some time in industry instead turned out to be immensely valuable in gaining some perspective on my career as a whole. In college, the prospect of eventually applying my mainly theoretical knowledge to problems in “the real world” always seemed intimidating. Working for a startup, I learned to overcome my (unsubstantiated) fear of failure, as well as to thrive in an environment that promoted autonomous decision making and creative problem solving. Being able to meaningfully contribute to an expert team of developers right from the beginning was an empowering experience that enabled me to grow not only professionally but personally as well.&lt;/p&gt;

&lt;p&gt;With newfound confidence and inspiration, I decided to give graduate school another try. And this time, I would come prepared!&lt;/p&gt;

&lt;p&gt;The first thing I did was to start studying for the GRE about half a year before applications were due. I even signed up for a small test preparation class that would meet once a week and ultimately help increase my GRE scores by a huge margin. Most importantly though, I had a much better understanding of why I &lt;em&gt;really&lt;/em&gt; wanted to graduate school: to gain an deeper understanding of state-of-the-art methodologies in NLP/ML, and to acquire the relevant skill set to carry out independent research in a large-scale, industrial setting. I also really missed the intellectual freedom of studying at a university and was hoping to get a chance to collaborate on a variety of different research projects in academia.&lt;/p&gt;

&lt;p&gt;After identifying a handful of suitable Masters instead of PhD programs, I was capable of composing a &lt;a href=&quot;http://www.cs.umd.edu/grad/writing-statement-of-pupose&quot;&gt;cohesive, original statement&lt;/a&gt; for each university which, in fact, clearly stated my purpose and motivation. I also made sure to devote more than half of each statement to the particular research groups and faculty I wanted to work with, and details on potential common areas of interest.&lt;/p&gt;

&lt;p&gt;Since I went through the process before, I was able to get ahead of all the common pitfalls and approach the application season level-headed and well prepared. Working a day job was additionally invaluable in that I had both the funds and enough spare time to put together a strong application. In the end, I was able to score an offer of admission from 2 out of the 5 universities I applied to, which to me is a very proud accomplishment.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;where-im-going&quot;&gt;Where I’m going&lt;/h3&gt;

&lt;p&gt;Time will tell! For now, I will be joining the MSCS program at NYU this fall. I am beyond to excited to finally go to graduate school, and I’m very much looking forward to starting a new chapter in New York City. I am hoping to write and publish more as I go along, so please don’t hesitate to share your own thoughts and experiences.&lt;/p&gt;

&lt;p&gt;To anyone still out there trying, know that it is indeed possible to overcome most challenges along the way through tenacity, perseverance, and hard work. Just keep at it and &lt;a href=&quot;http://www.chaosmatrix.org/library/humor/reject.html&quot;&gt;don’t let yourself get down&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Tue, 18 Apr 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/posts/thoughts-on-getting-into-graduate-school</link>
        <guid isPermaLink="true">http://localhost:4000/posts/thoughts-on-getting-into-graduate-school</guid>
      </item>
    
      <item>
        <title>Dish AI API</title>
        <description>&lt;p&gt;At &lt;a href=&quot;https://www.wayblazer.ai/&quot;&gt;WayBlazer&lt;/a&gt;, our product manager kept joking about how we needed an automated system (dubbed “dish AI”) to review our catered lunches every day. So I built it, featuring a preprocessed &lt;a href=&quot;https://www.yelp.com/dataset_challenge&quot;&gt;restaurant review data set&lt;/a&gt;, &lt;a href=&quot;https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html&quot;&gt;topic models&lt;/a&gt;, a &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_chain#Markov_text_generators&quot;&gt;Markov chain generator&lt;/a&gt;, and a &lt;a href=&quot;http://flask.pocoo.org/&quot;&gt;Flask API&lt;/a&gt; to put it all together!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/melanietosik/dish-ai&quot; class=&quot;pa3 tc ba br2 db&quot;&gt;View project on GitHub ☺&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Tue, 18 Apr 2017 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/posts/dish-ai</link>
        <guid isPermaLink="true">http://localhost:4000/posts/dish-ai</guid>
      </item>
    
      <item>
        <title>Semantic role labeling using CRF</title>
        <description>&lt;p&gt;My very last undergraduate project for a class on advanced language modeling, where we discussed the theoretical foundations of &lt;a href=&quot;https://en.wikipedia.org/wiki/Hidden_Markov_model&quot;&gt;hidden Markov models&lt;/a&gt; (HMM), the &lt;a href=&quot;https://en.wikipedia.org/wiki/Viterbi_algorithm&quot;&gt;Viterbi&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;EM&lt;/a&gt; algorithm, &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-linear_model&quot;&gt;log-linear models&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Principle_of_maximum_entropy#Maximum_entropy_models&quot;&gt;maximum entropy models&lt;/a&gt; (MEMM), and as well as &lt;a href=&quot;https://en.wikipedia.org/wiki/Conditional_random_field&quot;&gt;conditional random fields&lt;/a&gt; (CRF).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I finished the class with a short paper on &lt;a href=&quot;http://localhost:4000/files/srl.pdf&quot;&gt;“Semantic Role Labeling using Linear-Chain CRF”&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/melanietosik/srl&quot; class=&quot;pa3 tc ba br2 db&quot;&gt;View project on GitHub ☺&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Tue, 22 Sep 2015 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/posts/semantic-role-labeling-crf</link>
        <guid isPermaLink="true">http://localhost:4000/posts/semantic-role-labeling-crf</guid>
      </item>
    
      <item>
        <title>Sequence alignment in C++</title>
        <description>&lt;p&gt;During my senior year, I finally took a class on advanced C++. Surprisingly enough, it didn’t seem nearly as hard as the first one I had to struggle through a few years before, and I ended up having a lot of fun with it.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;As final project, I decided to work on edit distances and implemented the &lt;a href=&quot;https://en.wikipedia.org/wiki/Wagner%E2%80%93Fischer_algorithm&quot;&gt;Wagner–Fischer algorithm&lt;/a&gt; using dynamic programming. Later on, I expanded the project to also cover the &lt;a href=&quot;https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm&quot;&gt;Needleman-Wunsch algorithm&lt;/a&gt; for global sequence alignment.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/melanietosik/cpp2/blob/master/sda/README.md&quot; class=&quot;pa3 tc ba br2 db&quot;&gt;View project on GitHub ☺&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Mon, 17 Aug 2015 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/posts/edit-distances-and-sequence-alignment</link>
        <guid isPermaLink="true">http://localhost:4000/posts/edit-distances-and-sequence-alignment</guid>
      </item>
    
      <item>
        <title>Semantic dependency graph parsing</title>
        <description>&lt;p&gt;For a class on semantic dependency graph parsing, I wrote a &lt;a href=&quot;https://github.com/melanietosik/dependency-graph-parsing&quot;&gt;simple script&lt;/a&gt; that computes statistics for semantic dependency graphs. It also generates plots for the distribution of words per &lt;a href=&quot;http://en.wikipedia.org/wiki/Directed_graph#Indegree_and_outdegree&quot;&gt;indegree and outdegree&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;As final project, I submitted a comprehensive review on &lt;a href=&quot;http://localhost:4000/files/amr.pdf&quot;&gt;Abstract Meaning Representation (AMR)&lt;/a&gt; – a set of English sentences paired with simple, readable semantic representations.&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Thu, 09 Jul 2015 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/posts/semantic-dependency-graph-parsing</link>
        <guid isPermaLink="true">http://localhost:4000/posts/semantic-dependency-graph-parsing</guid>
      </item>
    
      <item>
        <title>String to semantic graph alignment</title>
        <description>&lt;p&gt;For my undergraduate thesis, I started working on semantic parsing: the problem of mapping natural language strings to meaning representations. In order to train a semantic parser for English to &lt;a href=&quot;https://amr.isi.edu/&quot;&gt;Abstract Meaning Representation&lt;/a&gt; (AMR), we first need to know which phrases in the input sentence invoked which concepts in the corresponding AMR graph. The project aimed at building an English/AMR aligner to solve this task automatically.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Here is my &lt;a href=&quot;http://localhost:4000/files/thesis.pdf&quot;&gt;final thesis&lt;/a&gt;, which was originally based on a paper titled &lt;a href=&quot;https://www.isi.edu/natural-language/mt/amr_eng_align.pdf&quot;&gt;“Aligning English Strings with Abstract Meaning Representation Graphs”&lt;/a&gt; (Pourdamghani et al., 2014).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/melanietosik/string-to-amr-alignment&quot; class=&quot;pa3 tc ba br2 db&quot;&gt;View project on GitHub ☺&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Mon, 30 Mar 2015 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/posts/string-to-semantic-graph-alignment</link>
        <guid isPermaLink="true">http://localhost:4000/posts/string-to-semantic-graph-alignment</guid>
      </item>
    
  </channel>
</rss>
